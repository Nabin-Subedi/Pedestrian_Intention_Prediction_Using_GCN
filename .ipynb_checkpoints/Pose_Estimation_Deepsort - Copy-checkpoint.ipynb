{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Estimation and tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "# from tf_pose import common\n",
    "# from tf_pose.estimator import TfPoseEstimator\n",
    "# from tf_pose.networks import get_graph_path, model_wh\n",
    "# from scripts.custom_functions import plot_img , get_human_pose, show_heatmap, show_vectormaps, show_keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from PIL import Image\n",
    "from deep_sort import preprocessing\n",
    "from deep_sort import nn_matching\n",
    "from deep_sort.detection import Detection\n",
    "from deep_sort.detection_yolo import Detection_YOLO\n",
    "from deep_sort.tracker import Tracker\n",
    "from tools import generate_detections as gdet\n",
    "import imutils.video\n",
    "# from videocaptureasync import VideoCaptureAsync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_end(bbox, candidates):\n",
    "    \"\"\"Computes intersection over union.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox : ndarray\n",
    "        A bounding box in format `(top left x, top left y, width, height)`.\n",
    "    candidates : ndarray\n",
    "        A matrix of candidate bounding boxes (one per row) in the same format\n",
    "        as `bbox`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ndarray\n",
    "        The intersection over union in [0, 1] between the `bbox` and each\n",
    "        candidate. A higher score means a larger fraction of the `bbox` is\n",
    "        occluded by the candidate.\n",
    "\n",
    "    \"\"\"\n",
    "    bbox_tl, bbox_br = bbox[:2], bbox[:2] + bbox[2:]\n",
    "    candidates_tl = candidates[:, :2]\n",
    "    candidates_br = candidates[:, :2] + candidates[:, 2:]\n",
    "\n",
    "    tl = np.c_[np.maximum(bbox_tl[0], candidates_tl[:, 0])[:, np.newaxis],\n",
    "               np.maximum(bbox_tl[1], candidates_tl[:, 1])[:, np.newaxis]]\n",
    "    br = np.c_[np.minimum(bbox_br[0], candidates_br[:, 0])[:, np.newaxis],\n",
    "               np.minimum(bbox_br[1], candidates_br[:, 1])[:, np.newaxis]]\n",
    "    wh = np.maximum(0., br - tl)\n",
    "\n",
    "    area_intersection = wh.prod(axis=1)\n",
    "    area_bbox = bbox[2:].prod()\n",
    "    area_candidates = candidates[:, 2:].prod(axis=1)\n",
    "    return area_intersection / (area_bbox + area_candidates - area_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T22:24:36.102767Z",
     "start_time": "2020-07-26T22:24:36.098389Z"
    }
   },
   "outputs": [],
   "source": [
    "# optional \n",
    "logger = logging.getLogger('TfPoseEstimator-WebCam')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and TfPose Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T21:22:37.234375Z",
     "start_time": "2020-08-01T21:22:37.230433Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-01-27 22:03:59,041] [TfPoseEstimator] [INFO] loading graph from E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n",
      "2021-01-27 22:03:59,041 INFO loading graph from E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\models\\graph/mobilenet_thin/graph_opt.pb(default size=432x368)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:311: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,045 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:311: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:312: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,048 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:312: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:330: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,080 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:330: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:332: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,232 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:332: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfPoseEstimator/MobilenetV1/Conv2d_0/weights\n",
      "TfPoseEstimator/image\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_0/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_1_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_2_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_3_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_4_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_5_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_6_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_7_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_8_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_9_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_10_pointwise/Relu\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_depthwise/depthwise_weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/weights\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_depthwise/depthwise\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/Conv2D\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/MobilenetV1/Conv2d_11_pointwise/Relu\n",
      "TfPoseEstimator/Conv2d_3_pool\n",
      "TfPoseEstimator/feat_concat/axis\n",
      "TfPoseEstimator/feat_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage1_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage2_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage3_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage4_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage5_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_concat/axis\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_concat\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L1_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_1_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_2_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_3_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_4_pointwise/Relu\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_depthwise/depthwise_weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/weights\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_depthwise/depthwise\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/Conv2D\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/Conv2D_bn_offset\n",
      "TfPoseEstimator/Openpose/MConv_Stage6_L2_5_pointwise/BatchNorm/FusedBatchNorm\n",
      "TfPoseEstimator/Openpose/concat_stage7/axis\n",
      "TfPoseEstimator/Openpose/concat_stage7\n",
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:341: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,362 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:341: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:342: The name tf.image.resize_area is deprecated. Please use tf.compat.v1.image.resize_area instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,367 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:342: The name tf.image.resize_area is deprecated. Please use tf.compat.v1.image.resize_area instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\tensblur\\smoother.py:96: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,393 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\tensblur\\smoother.py:96: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 22:03:59,436 WARNING From E:\\wuhan\\lab\\HumanPoseEstimation\\tf-pose-estimation\\tf_pose\\estimator.py:354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model= 'mobilenet_thin'#'cmu'\n",
    "resize='432x368'\n",
    "w, h = model_wh(resize)\n",
    "e = TfPoseEstimator(get_graph_path(model), target_size=(w, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path ='./videos/video_0001.mp4'#'./videos/peoples_short.mp4' #'./videos/karate_short.mp4'#'./videos/fall_short.mp4' #'./videos/test_peoples_480.mp4'#'./videos/karate_2.mp4'#'./videos/test.mkv'\n",
    "showBG = True # False to show skeleton only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_write_name,_ = os.path.splitext(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def draw_bbox(image, bboxes, show_label=True, show_confidence = True, Text_colors=(255,255,0), rectangle_colors='', tracking=False):   \n",
    "#     NUM_CLASS = {0: 'person'}\n",
    "#     num_classes = len(NUM_CLASS)\n",
    "#     image_h, image_w, _ = image.shape\n",
    "#     hsv_tuples = [(1.0 * x / num_classes, 1., 1.) for x in range(num_classes)]\n",
    "#     #print(\"hsv_tuples\", hsv_tuples)\n",
    "#     colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "#     colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
    "\n",
    "#     random.seed(0)\n",
    "#     random.shuffle(colors)\n",
    "#     random.seed(None)\n",
    "\n",
    "#     for i, bbox in enumerate(bboxes):\n",
    "#         coor = np.array(bbox[:4], dtype=np.int32)\n",
    "#         score = bbox[4]\n",
    "#         class_ind = int(bbox[5])\n",
    "#         bbox_color = rectangle_colors if rectangle_colors != '' else colors[class_ind]\n",
    "#         bbox_thick = int(0.6 * (image_h + image_w) / 1000)\n",
    "#         if bbox_thick < 1: bbox_thick = 1\n",
    "#         fontScale = 0.75 * bbox_thick\n",
    "#         (x1, y1), (x2, y2) = (coor[0], coor[1]), (coor[2], coor[3])\n",
    "\n",
    "#         # put object rectangle\n",
    "#         cv2.rectangle(image, (x1, y1), (x2, y2), bbox_color, bbox_thick*2)\n",
    "\n",
    "#         if show_label:\n",
    "#             # get text label\n",
    "#             score_str = \" {:.2f}\".format(score) if show_confidence else \"\"\n",
    "\n",
    "#             if tracking: score_str = \" \"+str(score)\n",
    "\n",
    "#             try:\n",
    "#                 label = \"{}\".format(NUM_CLASS[class_ind]) + score_str\n",
    "#             except KeyError:\n",
    "#                 print(\"You received KeyError, this might be that you are trying to use yolo original weights\")\n",
    "#                 print(\"while using custom classes, if using custom model in configs.py set YOLO_CUSTOM_WEIGHTS = True\")\n",
    "\n",
    "#             # get text size\n",
    "#             (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "#                                                                   fontScale, thickness=bbox_thick)\n",
    "#             # put filled text rectangle\n",
    "#             cv2.rectangle(image, (x1, y1), (x1 + text_width, y1 - text_height - baseline), bbox_color, thickness=cv2.FILLED)\n",
    "\n",
    "#             # put text above rectangle\n",
    "#             cv2.putText(image, label, (x1, y1-4), cv2.FONT_HERSHEY_COMPLEX_SMALL,\n",
    "#                         fontScale, Text_colors, bbox_thick, lineType=cv2.LINE_AA)\n",
    "\n",
    "#     return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_boxes(keypoints):\n",
    "    keypoints=keypoints[np.invert(keypoints[:,2]==0)]\n",
    "    #keypoints=keypoints[np.all(keypoints != 0, axis=1)] # for comparing if all three values in a row is equal to zero. \n",
    "    x1=keypoints[:,0].min()\n",
    "    y1=keypoints[:,1].min()\n",
    "    x2=keypoints[:,0].max()\n",
    "    y2=keypoints[:,1].max()\n",
    "    width=x2-x1\n",
    "    height=y2-y1\n",
    "    height=height+(0.08*height)\n",
    "    y1=max((y1-(0.08*height)),0)\n",
    "    bbox=[x1,y1,width,height]\n",
    "    return bbox\n",
    "\n",
    "def get_head_and_boxes(file_name):\n",
    "    bboxes=[]\n",
    "    head_cords=[]\n",
    "    with open (file_name) as f:\n",
    "        data= json .load(f)\n",
    "    for i in range(len(data['people'])):\n",
    "        keypoints=data['people'][i]['pose_keypoints_2d']\n",
    "        keypoints=np.array(keypoints)\n",
    "        keypoints=keypoints.reshape(25,3)\n",
    "        keypoints_array=keypoints.copy()\n",
    "        keypoints=keypoints.tolist()\n",
    "        if keypoints[0][2] < 0.15:\n",
    "            head_cord=(None,None)\n",
    "        else:\n",
    "            head_cord=(keypoints[0][0],keypoints[0][1])\n",
    "        head_cords.append(head_cord)\n",
    "        bbox=calculate_boxes(keypoints_array)\n",
    "        #bbox=[keypoints[0][0]-10,keypoints[0][1]-10,25,25]\n",
    "        bboxes.append(bbox)\n",
    "#     print(f'number of poeple: {len(data[\"people\"])}')\n",
    "#     print(bboxes)\n",
    "    return bboxes,head_cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_humans_full_body():\n",
    "    ##Getting full body co-ordinate for number of humans detected\n",
    "    humans_full_body=[]\n",
    "    bboxes=[]\n",
    "    head_cords=[]\n",
    "    for human in humans:\n",
    "        full_body={}\n",
    "        body_cord=human.get_upper_body_box(img_w=image.shape[1], img_h=image.shape[0])\n",
    "        if body_cord==None:\n",
    "            continue\n",
    "        ## for full body bounding box ##\n",
    "        x1=int(body_cord['x']-body_cord['w']//1.1)\n",
    "        y1=int(body_cord['y']-body_cord['h']//1.8)\n",
    "        x2=int(body_cord['x']+body_cord['w']//1.1)\n",
    "        y2=int(body_cord['y']+body_cord['h']*1.5)\n",
    "        \n",
    "        ##for upper body bounding box##\n",
    "#         x1=int(body_cord['x']-body_cord['w']//2)\n",
    "#         y1=int(body_cord['y']-body_cord['h']//2)\n",
    "#         x2=int(body_cord['x']+body_cord['w']//2)\n",
    "#         y2=int(body_cord['y']+body_cord['h']//2)\n",
    "        \n",
    "        # Comparing so that the co-ordinate doesnot go out of bound\n",
    "        x1=max(x1,0)\n",
    "        y1=max(y1,0)\n",
    "        x2=min(x2,image.shape[1]-2)\n",
    "        y2=min(y2,image.shape[0]-2)\n",
    "        full_body['x1']=x1\n",
    "        full_body['y1']=y1\n",
    "        full_body['x2']=x2\n",
    "        full_body['y2']=y2\n",
    "        humans_full_body.append(full_body)\n",
    "        # for deep sort getting x,y,width,height\n",
    "        width=x2-x1\n",
    "        height=y2-y1\n",
    "        bbox=[x1,y1,width,height]\n",
    "        bboxes.append(bbox)\n",
    "        \n",
    "        # getting head_cord(nose) based on score for fall detection\n",
    "        try:\n",
    "            head_score=human.body_parts[0].score # if score of nose is less than 0.25 ignoring.\n",
    "            if head_score < 0.25:\n",
    "                head_cord=(None,None)\n",
    "            else:\n",
    "                a=human.body_parts[0] # human.body_parts[0] is for nose coordinates\n",
    "                x = a.x*image.shape[1]   # x coordinate relative to image. Note: image shall be passed. \n",
    "                y = a.y*image.shape[0]\n",
    "                head_cord=(x,y) \n",
    "        except:\n",
    "            head_cord=(None,None)\n",
    "            \n",
    "        head_cords.append(head_cord)\n",
    "        \n",
    "    #bboxes=np.array(bboxes) # uncomment to convert list to array.\n",
    "    return humans_full_body,bboxes,head_cords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_humans_face():\n",
    "    face_boxes=[]\n",
    "    bboxes=[]\n",
    "    for human in humans:\n",
    "        face={}\n",
    "        face_cord=human.get_face_box(img_w=image.shape[1], img_h=image.shape[0])\n",
    "        if face_cord==None:\n",
    "            continue\n",
    "        x1=int(face_cord['x']-face_cord['w']//2)\n",
    "        y1=int(face_cord['y']-face_cord['h']//2)\n",
    "        x2=int(face_cord['x']+face_cord['w']//2)\n",
    "        y2=int(face_cord['y']+face_cord['h']//2)\n",
    "        face['x1']=x1\n",
    "        face['y1']=y1\n",
    "        face['x2']=x2\n",
    "        face['y2']=y2\n",
    "        face_boxes.append(face)\n",
    "        # for deep sort getting x,y,width,height\n",
    "        width=x2-x1\n",
    "        height=y2-y1\n",
    "        bbox=[x1,y1,width,height]\n",
    "        bboxes.append(bbox)\n",
    "    return face_boxes,bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head_cord(bbox,boxes,head_cords):\n",
    "    '''\n",
    "    Comparing tracked boxes with original boxes based on iou \n",
    "    and getting head co-ordinate of that person in paricular bounding box.\n",
    "    Note: head_cord in'head_cords' for each person is in same position as bounding box in 'boxes'  \n",
    "    '''\n",
    "    iou_values=iou_end(bbox, boxes)\n",
    "    iou_dict={'position':np.argmax(iou_values),'iou_value':np.max(iou_values)}\n",
    "    if iou_dict['iou_value'] >0.80:\n",
    "        head_cord=head_cords[iou_dict['position']]\n",
    "    else:\n",
    "        head_cord=(None,None)\n",
    "    return head_cord\n",
    "\n",
    "def store_data_for_fall(data_for_fall,person_id,frame_num,head_cord):\n",
    "    ''' Updates data_for_fall. Stores new data and pops old data.\n",
    "        Stores 5 data for each id.\n",
    "    '''\n",
    "    if person_id in data_for_fall.keys():\n",
    "        data_for_fall[person_id].append({'head_cord':head_cord, 'frame_number': frame_num})\n",
    "        if len(data_for_fall[person_id])>5:\n",
    "            data_for_fall[person_id].pop(0)\n",
    "    else:\n",
    "        data_for_fall[person_id]=[{'head_cord':(None,None),'frame_number':0}]\n",
    "        for _ in range(2):\n",
    "            data_for_fall[person_id].append({'head_cord':(None,None),'frame_number':0})\n",
    "        data_for_fall[person_id].append({'head_cord':head_cord,'frame_number':frame_num})\n",
    "\n",
    "        \n",
    "def detect_fall(data_for_fall,person_id,fall_count,id_to_ignore):\n",
    "    prev_frame_num=data_for_fall[person_id][-4]['frame_number'] #change for other frame\n",
    "    current_frame_num=data_for_fall[person_id][-1]['frame_number']\n",
    "    if current_frame_num-prev_frame_num > 4:\n",
    "        return fall_count,id_to_ignore\n",
    "    prev_cord=data_for_fall[person_id][-4]['head_cord'] #change for other frame\n",
    "    current_cord=data_for_fall[person_id][-1]['head_cord']\n",
    "    prev_cord_y=prev_cord[1]\n",
    "    current_cord_y=current_cord[1]\n",
    "    if (current_cord_y==None) or ( prev_cord_y==None):\n",
    "        return fall_count,id_to_ignore\n",
    "    distance_per_frame={1:10,2:13,3:30,4:35}\n",
    "    if current_cord_y-prev_cord_y > distance_per_frame[current_frame_num-prev_frame_num]:\n",
    "        print(f'Fall Detected for id:{person_id} at frame:{current_frame_num}') \n",
    "        print(f'Distance:{distance_per_frame[current_frame_num-prev_frame_num]}')\n",
    "        fall_count+=1\n",
    "        id_to_ignore[0].append(person_id)\n",
    "        id_to_ignore[1].append(current_frame_num+25) # stores number of frame to ignore for that id.\n",
    "    return fall_count,id_to_ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if cap.isOpened() is False:\n",
    "    print(\"Error opening video stream or file\")\n",
    "fpss = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(fpss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video was successfully saved\n"
     ]
    }
   ],
   "source": [
    "###WITHOUT DEEPSORT OBJECT TRACKING ###\n",
    "\n",
    "count = 0\n",
    "fall_count=0\n",
    "y1 = [0,0,0,0]\n",
    "frame = 0\n",
    "fps_time = 0\n",
    "new_fall=True\n",
    "frame_to_ignore=0\n",
    "while True:\n",
    "    ret_val, image = cap.read()\n",
    "    i =1\n",
    "    count+=1\n",
    "    if not ret_val:\n",
    "        break\n",
    "    \n",
    "\n",
    "    humans = e.inference(image,\n",
    "                         resize_to_default=(w > 0 and h > 0),\n",
    "                         upsample_size=4.0)\n",
    "    if not showBG:\n",
    "        image = np.zeros(image.shape)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    ## Drawing full body bounding box\n",
    "    humans_full_body,bboxes= get_humans_full_body() # add arguments humans and image to pass if function not in same global variable scope.\n",
    "    for human_full_body in humans_full_body:\n",
    "        image=cv2.rectangle(image,\n",
    "                           (human_full_body['x1'],\n",
    "                            human_full_body['y1']),\n",
    "                           (human_full_body['x2'],\n",
    "                            human_full_body['y2']),\n",
    "                           [0,255,255],2)\n",
    "    ## Drawing face bounding box\n",
    "#     face_boxes= get_humans_face() # add arguments humans and image to pass if function not in same global variable scope.\n",
    "#     for face_box in face_boxes:\n",
    "#         image=cv2.rectangle(image,\n",
    "#                            (face_box['x1'],\n",
    "#                             face_box['y1']),\n",
    "#                            (face_box['x2'],\n",
    "#                             face_box['y2']),\n",
    "#                            [0,255,255],2)\n",
    "    ## For fall detection            \n",
    "#     for human in humans:\n",
    "#         # we select one person from num of person\n",
    "#         for i in range(len(humans)):\n",
    "#             try:\n",
    "#                 #human.parts contains all the detected body parts\n",
    "#                 a = human.body_parts[0]   # human.body_parts[0] is for head point coordinates\n",
    "#                 x = a.x*image.shape[1]   # x coordinate relative to image \n",
    "#                 y = a.y*image.shape[0]   # y coordinate relative to image\n",
    "#                 y1.append(y)   # store value of y coordinate in list to compare two frames\n",
    "#             except:\n",
    "#                 pass\n",
    "#             if count > 4: #Ignoring first few frames\n",
    "                                \n",
    "#                 cv2.putText(image, f\"Fall Detected Count: {fall_count}\", (20,50), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255), \n",
    "#                         2, 11)\n",
    "#                 #cv2.putText(image, f'{fall_count}', (20,80), cv2.FONT_HERSHEY_COMPLEX, 1.1, (0,0,255),2, 11)\n",
    "#                 if ((y - y1[-2]) > 25) and (new_fall==True) :  # it's distance between frame and comparing it with thresold value\n",
    "#                     new_fall=False\n",
    "#                     frame_to_ignore=count+24\n",
    "#                     fall_count+=1\n",
    "#                     #cv2.putText(image, f'{fall_count}', (30,60), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0,255), \n",
    "#                     #    2, 11)\n",
    "#                     print(\"fall detected.\",i+1, count)#You can set count for get that your detection is working\n",
    "#                 if count == frame_to_ignore:\n",
    "#                     new_fall=True\n",
    "                    \n",
    "\n",
    "    \n",
    "    ## putting fps text and displaying\n",
    "    cv2.putText(image, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.imshow('tf-pose-estimation result', image)\n",
    "    fps_time = time.time()\n",
    "    \n",
    "    ### For Saving video ###    \n",
    "#     if frame == 0:   # It's use to intialize video writer ones\n",
    "#         out = cv2.VideoWriter(file_write_name+'_output.avi',cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "#                 20,(image.shape[1],image.shape[0]))\n",
    "#         frame+=1\n",
    "#     out.write(image)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"The video was successfully saved\")\n",
    "#logger.debug('finished+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video was successfully saved\n"
     ]
    }
   ],
   "source": [
    "### USING TF-POSE ESTIMATOR###\n",
    "\n",
    "data_for_fall={}\n",
    "id_to_ignore=([],[])\n",
    "\n",
    "count = 0\n",
    "fall_count=0\n",
    "y1 = [0,0,0,0]\n",
    "frame_num = 0\n",
    "fps_time = 0\n",
    "new_fall=True\n",
    "\n",
    "### for deep_sort ####\n",
    "max_cosine_distance = 0.7\n",
    "nn_budget = None\n",
    "nms_max_overlap = 1.0\n",
    "#initialize deep sort object\n",
    "model_filename = 'model_data/mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)\n",
    "\n",
    "tracking = True\n",
    "writeVideo_flag = False\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret_val, image = cap.read()\n",
    "    if not ret_val:\n",
    "        break\n",
    "    frame=image.copy()\n",
    "    i =1\n",
    "    count+=1\n",
    "    if not ret_val:\n",
    "        break\n",
    "    \n",
    "\n",
    "    humans = e.inference(image,\n",
    "                         resize_to_default=(w > 0 and h > 0),\n",
    "                         upsample_size=4.0)\n",
    "    if not showBG:\n",
    "        image = np.zeros(image.shape)\n",
    "    image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "    \n",
    "    ## Drawing body bounding box\n",
    "    humans_full_body,boxes,head_cords= get_humans_full_body() # add arguments humans and image to pass if function not in same global variable scope.\n",
    "\n",
    "#     for human_full_body in humans_full_body:\n",
    "#         image=cv2.rectangle(image,\n",
    "#                            (human_full_body['x1'],\n",
    "#                             human_full_body['y1']),\n",
    "#                            (human_full_body['x2'],\n",
    "#                             human_full_body['y2']),\n",
    "#                            [0,255,255],2)\n",
    "\n",
    "        ## Drawing face bounding box\n",
    "#     face_boxes,boxes= get_humans_face() # add arguments humans and image to pass if function not in same global variable scope.\n",
    "#     for face_box in face_boxes:\n",
    "#         image=cv2.rectangle(image,\n",
    "#                            (face_box['x1'],\n",
    "#                             face_box['y1']),\n",
    "#                            (face_box['x2'],\n",
    "#                             face_box['y2']),\n",
    "#                            [0,255,255],2)\n",
    "    \n",
    "    #added by me \n",
    "    #print(f'Boxes:{boxes}')\n",
    "    boxes.reverse()\n",
    "    \n",
    "    boxes_for_error=boxes.copy()\n",
    "    \n",
    "    classes=np.ones(len(boxes),dtype='<U6')\n",
    "    classes[classes=='1']='person'\n",
    "    classes=classes.tolist()\n",
    "    confidence=np.ones(len(boxes))*0.9\n",
    "    confidence=confidence.tolist()\n",
    "    #print(f'Boxes:{boxes} \\nConfidence:{confidence} \\nClasses:{classes}')\n",
    "    #print(f'Boxes:{type(boxes)} \\nConfidence:{type(confidence)} \\nClasses:{type(classes)}')\n",
    "    \n",
    "    if tracking:\n",
    "        features = encoder(image, boxes)#encoder(frame, boxes)\n",
    "\n",
    "        detections = [Detection(bbox, confidence, cls, feature) for bbox, confidence, cls, feature in\n",
    "                      zip(boxes, confidence, classes, features)]\n",
    "    else:\n",
    "        detections = [Detection_YOLO(bbox, confidence, cls) for bbox, confidence, cls in\n",
    "                      zip(boxes, confidence, classes)]\n",
    "\n",
    "    # Run non-maxima suppression.\n",
    "    boxes = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = det.to_tlbr()\n",
    "        score = \"%.2f\" % round(det.confidence * 100, 2) + \"%\"\n",
    "        cv2.rectangle(image, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
    "        if len(classes) > 0:\n",
    "            cls = det.cls\n",
    "            cv2.putText(image, str(cls) + \" \" + score, (int(bbox[0]), int(bbox[3])), 0,\n",
    "                        1e-3 * frame.shape[0], (255, 255, 0), 2)\n",
    "        \n",
    "        #print(f'Detection:{det.to_tlbr()}')\n",
    "        #print(f'Detection type:{type(det.to_tlbr())}')\n",
    "        #print(f'Detection 1st item:{det.to_tlbr()[0]}')\n",
    "\n",
    "    if tracking:\n",
    "        # Call the tracker\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "            cv2.rectangle(image, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 255, 255), 2)\n",
    "            cv2.putText(image, \"ID: \" + str(track.track_id), (int(bbox[0]), int(bbox[1])), 0,\n",
    "                        1e-3 * frame.shape[0], (255, 255, 0), 2)\n",
    "            \n",
    "            person_id=track.track_id\n",
    "            if boxes.size == 0:\n",
    "                break\n",
    "            head_cord=get_head_cord(track.to_tlwh(),boxes,head_cords) \n",
    "            #print(f'Head cord:{head_cord}')\n",
    "            store_data_for_fall(data_for_fall,person_id,frame_num,head_cord)\n",
    "            if not person_id in id_to_ignore[0]:\n",
    "                fall_count,id_to_ignore = detect_fall(data_for_fall,person_id,fall_count,id_to_ignore)\n",
    "            if frame_num in id_to_ignore[1]:\n",
    "                id_to_ignore[0].pop(0)\n",
    "                id_to_ignore[1].pop(0)\n",
    "            #print(f'Tracked Id:{track.track_id}')\n",
    "            #print(f'Tracked Detection:{track.to_tlwh()}')\n",
    "            #print(f'Tracked Detection type:{type(track.to_tlbr())}')\n",
    "    #print(f'Fall Count:{fall_count}')\n",
    "\n",
    "    ## For fall detection            \n",
    "#     for human in humans:\n",
    "#         # we select one person from num of person\n",
    "#         for i in range(len(humans)):\n",
    "#             try:\n",
    "#                 #human.parts contains all the detected body parts\n",
    "#                 a = human.body_parts[0]   # human.body_parts[0] is for head point coordinates\n",
    "#                 x = a.x*image.shape[1]   # x coordinate relative to image \n",
    "#                 y = a.y*image.shape[0]   # y coordinate relative to image\n",
    "#                 y1.append(y)   # store value of y coordinate in list to compare two frames\n",
    "#             except:\n",
    "#                 pass\n",
    "#             if count > 4: #Ignoring first few frames\n",
    "                                \n",
    "#                 cv2.putText(image, f\"Fall Detected Count: {fall_count}\", (20,50), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255), \n",
    "#                         2, 11)\n",
    "#                 #cv2.putText(image, f'{fall_count}', (20,80), cv2.FONT_HERSHEY_COMPLEX, 1.1, (0,0,255),2, 11)\n",
    "#                 if ((y - y1[-2]) > 25) and (new_fall==True) :  # it's distance between frame and comparing it with thresold value\n",
    "#                     new_fall=False\n",
    "#                     frame_to_ignore=count+24\n",
    "#                     fall_count+=1\n",
    "#                     #cv2.putText(image, f'{fall_count}', (30,60), cv2.FONT_HERSHEY_COMPLEX, 1.5, (0,0,255), \n",
    "#                     #    2, 11)\n",
    "#                     print(\"fall detected.\",i+1, count)#You can set count for get that your detection is working\n",
    "#                 if count == frame_to_ignore:\n",
    "#                     new_fall=True\n",
    "                    \n",
    "\n",
    "    \n",
    "    ## putting fps text and displaying\n",
    "    cv2.putText(image, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.putText(image, f\"Fall Detected Count: {fall_count}\", (20,50), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255), \n",
    "                       2)\n",
    "    #print(f'FPS:{1.0 / (time.time() - fps_time)}')\n",
    "    cv2.imshow('tf-pose-estimation result', image)\n",
    "    fps_time = time.time()\n",
    "    \n",
    "    ### For Saving video ###\n",
    "    if writeVideo_flag:\n",
    "        if frame_num == 0:   # It's use to intialize video writer ones\n",
    "            out = cv2.VideoWriter(file_write_name+'_output.avi',cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "                    20,(image.shape[1],image.shape[0]))\n",
    "        out.write(image)\n",
    "        \n",
    "    frame_num+=1    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"The video was successfully saved\")\n",
    "#logger.debug('finished+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall Detected for id:1 at frame:16\n",
      "Distance:30\n",
      "The video was successfully saved\n"
     ]
    }
   ],
   "source": [
    "### USING CAFFE JSON FILES###\n",
    "\n",
    "data_for_fall={}\n",
    "id_to_ignore=([],[])\n",
    "\n",
    "fall_count=0\n",
    "frame_num = 0\n",
    "fps_time = 0\n",
    "new_fall=True\n",
    "\n",
    "### for deep_sort ####\n",
    "max_cosine_distance = 0.7\n",
    "nn_budget = None\n",
    "nms_max_overlap = 1.0 # means if the iou value is less than 'the value' the objects are different else same. \n",
    "#initialize deep sort object\n",
    "model_filename = 'model_data/mars-small128.pb'\n",
    "encoder = gdet.create_box_encoder(model_filename, batch_size=1)\n",
    "metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "tracker = Tracker(metric)\n",
    "\n",
    "tracking = True\n",
    "writeVideo_flag = True\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret_val, image = cap.read()\n",
    "    if not ret_val:\n",
    "        break\n",
    "    frame=image.copy()\n",
    "    \n",
    "    file_num='000000000000'+str(frame_num)\n",
    "    file_num=file_num[-12:]\n",
    "    file_name='./videos/from caffe/json/video_0001/video_0001_'+file_num+'_keypoints.json'\n",
    "\n",
    "    ## Drawing body bounding box\n",
    "    boxes,head_cords=get_head_and_boxes(file_name)\n",
    "    boxes.reverse()\n",
    "    \n",
    "    classes=np.ones(len(boxes),dtype='<U6')\n",
    "    classes[classes=='1']='person'\n",
    "    classes=classes.tolist()\n",
    "    confidence=np.ones(len(boxes))*0.9\n",
    "    confidence=confidence.tolist()\n",
    "    #print(f'Boxes:{boxes} \\nConfidence:{confidence} \\nClasses:{classes}')\n",
    "    #print(f'Boxes:{type(boxes)} \\nConfidence:{type(confidence)} \\nClasses:{type(classes)}')\n",
    "    \n",
    "    if tracking:\n",
    "        features = encoder(image, boxes)#encoder(frame, boxes)\n",
    "\n",
    "        detections = [Detection(bbox, confidence, cls, feature) for bbox, confidence, cls, feature in\n",
    "                      zip(boxes, confidence, classes, features)]\n",
    "    else:\n",
    "        detections = [Detection_YOLO(bbox, confidence, cls) for bbox, confidence, cls in\n",
    "                      zip(boxes, confidence, classes)]\n",
    "\n",
    "    # Run non-maxima suppression.\n",
    "    boxes = np.array([d.tlwh for d in detections])\n",
    "    scores = np.array([d.confidence for d in detections])\n",
    "    indices = preprocessing.non_max_suppression(boxes, nms_max_overlap, scores)\n",
    "    detections = [detections[i] for i in indices]\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = det.to_tlbr()\n",
    "        score = \"%.2f\" % round(det.confidence * 100, 2) + \"%\"\n",
    "        cv2.rectangle(image, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 0, 0), 2)\n",
    "        if len(classes) > 0:\n",
    "            cls = det.cls\n",
    "            cv2.putText(image, str(cls) + \" \" + score, (int(bbox[0]), int(bbox[3])), 0, 1e-3 * frame.shape[0], (255, 255, 0), 2)\n",
    "        \n",
    "        #print(f'Detection:{det.to_tlbr()}')\n",
    "        #print(f'Detection type:{type(det.to_tlbr())}')\n",
    "        #print(f'Detection 1st item:{det.to_tlbr()[0]}')\n",
    "\n",
    "    if tracking:\n",
    "        # Call the tracker\n",
    "        tracker.predict()\n",
    "        tracker.update(detections)\n",
    "\n",
    "        for track in tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "            cv2.rectangle(image, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (255, 255, 255), 2)\n",
    "            cv2.putText(image, \"ID: \" + str(track.track_id), (int(bbox[0]), int(bbox[1])), 0,\n",
    "                        1e-3 * frame.shape[0], (255, 255, 0), 2)\n",
    "            \n",
    "            person_id=track.track_id\n",
    "\n",
    "            if boxes.size == 0:\n",
    "                break\n",
    "            head_cord=get_head_cord(track.to_tlwh(),boxes,head_cords) \n",
    "            store_data_for_fall(data_for_fall,person_id,frame_num,head_cord)\n",
    "            if not person_id in id_to_ignore[0]:\n",
    "                fall_count,id_to_ignore = detect_fall(data_for_fall,person_id,fall_count,id_to_ignore)\n",
    "            if frame_num in id_to_ignore[1]:\n",
    "                id_to_ignore[0].pop(0)\n",
    "                id_to_ignore[1].pop(0)\n",
    "            #print(f'Tracked Id:{track.track_id}')\n",
    "            #print(f'Tracked Detection:{track.to_tlwh()}')\n",
    "            #print(f'Tracked Detection type:{type(track.to_tlbr())}')\n",
    "    #print(f'Fall Count:{fall_count}')\n",
    "    \n",
    "    ####################remove it's for bbox of jaad and openpose testing#######\n",
    "    jaad_bboxes = get_bbox(frame_num)\n",
    "    for bbox in jaad_bboxes:\n",
    "        cv2.rectangle(image, (int(float(bbox[0])), int(float(bbox[1]))), (int(float(bbox[2])), int(float(bbox[3]))), (0, 0, 255), 2)\n",
    "    \n",
    "    #############################\n",
    "\n",
    "    ## putting fps text and displaying\n",
    "    cv2.putText(image, \"FPS: %f\" % (1.0 / (time.time() - fps_time)), (10, 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.putText(image, f\"Fall Detected Count: {fall_count}\", (20,50), cv2.FONT_HERSHEY_COMPLEX, 0.8, (0,0,255), \n",
    "                       2)\n",
    "    #print(f'FPS:{1.0 / (time.time() - fps_time)}')\n",
    "    cv2.imshow('tf-pose-estimation result', image)\n",
    "    fps_time = time.time()\n",
    "    \n",
    "    ### For Saving video ###\n",
    "    if writeVideo_flag:\n",
    "        if frame_num == 0:   # It's use to intialize video writer ones\n",
    "            out = cv2.VideoWriter(file_write_name+'_output.avi',cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),\n",
    "                    20,(image.shape[1],image.shape[0]))\n",
    "        out.write(image)\n",
    "        \n",
    "    frame_num+=1    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "#     if frame_num==4:\n",
    "#         break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"The video was successfully saved\")\n",
    "#logger.debug('finished+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Gcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pandas as pd\n",
    "# import pickle\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Conv2D,Flatten,Input,Activation,Dropout,BatchNormalization,GlobalAveragePooling2D,Lambda,Layer,PReLU,LeakyReLU\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_pickle('GCN/c_nc_averaged_dropped_null.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {\"C\": 1, \"NC\": 0}\n",
    "df = df.rename(columns={\"keypoints\": \"X\", \"C/NC\": \"Y\"})\n",
    "df[\"Y\"] = df.Y.apply(lambda x: lookup.get(x))\n",
    "X = np.stack(list(df[\"X\"]))\n",
    "X = X.reshape(-1, 300, 25, 2)\n",
    "X[:, :, :, 0] /= 1920.0\n",
    "X[:, :, :, 1] /= 1080.0\n",
    "Y=df['Y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[    \n",
    "    [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "    [1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,  0,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  1,  1,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0],    \n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0],    \n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1]\n",
    "    \n",
    "]\n",
    "A = np.array(A , dtype='float')\n",
    "G= nx.from_numpy_matrix(A)\n",
    "A= nx.normalized_laplacian_matrix(G).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_einsum(x,temporal=300):\n",
    "  A_stack = np.stack([A for _ in range(temporal)])\n",
    "  A_stack = tf.convert_to_tensor(A_stack,dtype='float')\n",
    "  x=tf.transpose(x,perm=[0,3,1,2])\n",
    "  x=tf.einsum('nctv,tvw->nctw', x, A_stack)\n",
    "  x=tf.transpose(x,perm=[0,2,3,1])\n",
    "  return x\n",
    "\n",
    "def change_shape(x):\n",
    "  x=tf.transpose(x,perm=[0,1,3,2])\n",
    "  return x\n",
    "\n",
    "\n",
    "i=Input(shape=(300,25,2))\n",
    "\n",
    "x=Lambda(get_einsum , arguments={'temporal':300})(i)\n",
    "x=Conv2D(filters=64,kernel_size=(3,3),padding='same')(x)\n",
    "x=LeakyReLU()(x)\n",
    "# x=Activation('relu')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "\n",
    "x=Lambda(get_einsum , arguments={'temporal':300})(x)\n",
    "x=Conv2D(filters=32,kernel_size=(3,3),padding='same')(x)\n",
    "x=LeakyReLU()(x)\n",
    "# x=Activation('relu')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "\n",
    "x=Lambda(get_einsum , arguments={'temporal':300})(x)\n",
    "x=Conv2D(filters=16,kernel_size=(5,5),padding='same')(x)\n",
    "x=LeakyReLU()(x)\n",
    "x=Dropout(0.2)(x)\n",
    "\n",
    "x=Lambda(change_shape)(x)\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "\n",
    "x=Dense(units=10)(x)\n",
    "x=LeakyReLU()(x)\n",
    "# x=Activation('relu')(x)\n",
    "x=Dropout(0.2)(x)\n",
    "x=Dense(units=1)(x)\n",
    "x=Activation('sigmoid')(x)\n",
    "\n",
    "gcn_model=Model(inputs=i,outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_model.load_weights('GCN/my_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nbnsu\\anaconda3\\envs\\tfpose\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction=gcn_model.predict(X_test)\n",
    "prediction=prediction>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.78      0.85        37\n",
      "           1       0.84      0.96      0.90        45\n",
      "\n",
      "    accuracy                           0.88        82\n",
      "   macro avg       0.89      0.87      0.87        82\n",
      "weighted avg       0.88      0.88      0.88        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410, 300, 25, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99234573, 0.        , 0.35136203, 0.        , 0.        ,\n",
       "       0.00553118])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(temp2.shape[0]):\n",
    "    iou_values=iou_end(temp2[i], temp1)\n",
    "    iou_dict={'position':np.argmax(iou_values),'iou_value':np.max(iou_values)}\n",
    "    \n",
    "    break\n",
    "iou_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [{'head_cord': (None, None), 'frame_number': 132},\n",
       "  {'head_cord': (None, None), 'frame_number': 143},\n",
       "  {'head_cord': (None, None), 'frame_number': 144},\n",
       "  {'head_cord': (None, None), 'frame_number': 145},\n",
       "  {'head_cord': (None, None), 'frame_number': 146}],\n",
       " 3: [{'head_cord': (651.703, 298.026), 'frame_number': 214},\n",
       "  {'head_cord': (650.43, 298.046), 'frame_number': 215},\n",
       "  {'head_cord': (693.529, 288.976), 'frame_number': 216},\n",
       "  {'head_cord': (690.949, 290.229), 'frame_number': 217},\n",
       "  {'head_cord': (535.627, 288.965), 'frame_number': 218}],\n",
       " 4: [{'head_cord': (689.61, 291.534), 'frame_number': 214},\n",
       "  {'head_cord': (693.511, 290.24), 'frame_number': 215},\n",
       "  {'head_cord': (539.519, 294.148), 'frame_number': 216},\n",
       "  {'head_cord': (538.195, 292.848), 'frame_number': 217},\n",
       "  {'head_cord': (None, None), 'frame_number': 218}],\n",
       " 8: [{'head_cord': (None, None), 'frame_number': 131},\n",
       "  {'head_cord': (None, None), 'frame_number': 132},\n",
       "  {'head_cord': (None, None), 'frame_number': 133},\n",
       "  {'head_cord': (None, None), 'frame_number': 134},\n",
       "  {'head_cord': (None, None), 'frame_number': 135}],\n",
       " 10: [{'head_cord': (547.315, 295.424), 'frame_number': 214},\n",
       "  {'head_cord': (543.434, 295.468), 'frame_number': 215},\n",
       "  {'head_cord': (646.554, 296.774), 'frame_number': 216},\n",
       "  {'head_cord': (None, None), 'frame_number': 217},\n",
       "  {'head_cord': (692.198, 290.284), 'frame_number': 218}]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={'position':np.argmax(iou_values),'iou_value':np.max(iou_values)}\n",
    "a['position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=None\n",
    "i=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempp=[[1,2,6],[2,55,63],[2,56,25],[32,5,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 5, 3], [2, 56, 25], [2, 55, 63], [1, 2, 6]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempp.reverse()\n",
    "tempp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': 401, 'y': 206, 'w': 26, 'h': 47},\n",
       " {'x': 125, 'y': 392, 'w': 36, 'h': 77},\n",
       " {'x': 348, 'y': 125, 'w': 31, 'h': 37},\n",
       " {'x': 739, 'y': 297, 'w': 31, 'h': 52},\n",
       " {'x': 132, 'y': 115, 'w': 26, 'h': 62},\n",
       " None,\n",
       " {'x': 749, 'y': 433, 'w': 28, 'h': 17}]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_cordss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'x': 401, 'y': 206, 'w': 26, 'h': 47},\n",
       " {'x': 125, 'y': 392, 'w': 36, 'h': 77},\n",
       " {'x': 348, 'y': 125, 'w': 31, 'h': 37},\n",
       " {'x': 739, 'y': 297, 'w': 31, 'h': 52},\n",
       " {'x': 132, 'y': 115, 'w': 26, 'h': 62},\n",
       " {'x': 749, 'y': 433, 'w': 28, 'h': 17}]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body_cordss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_fall={}\n",
    "for i in range (5):\n",
    "    data_for_fall[i]=[{'head_cord':(i,2),'frame_numnber':i}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_fall[1].append({'head_cord': (11, 2), 'frame_numnber': 1})\n",
    "if len(data_for_fall[1])>4:\n",
    "    data_for_fall[1].pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_fall[1][-1]['head_cord'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#temp={1:'hello','1':'hi'}\n",
    "temp={1:20,2:25,3:30,4:40}\n",
    "temp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "del temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=([],[])\n",
    "not 1 in temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 1, 8, 1]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    temp.pop(0)\n",
    "except:\n",
    "    pass\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(temp):\n",
    "    temp=2\n",
    "    print(temp)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=change(temp)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "if( temp==0) or (tempp==2):\n",
    "    print('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019762516021728516\n"
     ]
    }
   ],
   "source": [
    "st=time.time()\n",
    "with open ('./videos/from caffe/json/test_peoples_480/test_peoples_480_000000000000_keypoints.json') as f:\n",
    "    data= json .load(f)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['version', 'people'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 98.3419,\n",
       " 200.21,\n",
       " 0.895906,\n",
       " 119.261,\n",
       " 201.441,\n",
       " 0.887047,\n",
       " 127.123,\n",
       " 234.102,\n",
       " 0.867268,\n",
       " 115.339,\n",
       " 245.829,\n",
       " 0.192455,\n",
       " 77.4359,\n",
       " 198.867,\n",
       " 0.851245,\n",
       " 67.0465,\n",
       " 230.189,\n",
       " 0.8527,\n",
       " 67.1131,\n",
       " 255.045,\n",
       " 0.900241,\n",
       " 98.3122,\n",
       " 265.413,\n",
       " 0.809133,\n",
       " 110.079,\n",
       " 265.421,\n",
       " 0.807436,\n",
       " 108.801,\n",
       " 308.487,\n",
       " 0.900255,\n",
       " 104.917,\n",
       " 356.823,\n",
       " 0.765025,\n",
       " 84.0113,\n",
       " 265.394,\n",
       " 0.747431,\n",
       " 87.8788,\n",
       " 309.804,\n",
       " 0.856707,\n",
       " 87.9881,\n",
       " 362.026,\n",
       " 0.853963,\n",
       " 110.157,\n",
       " 175.411,\n",
       " 0.0933493,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 108.789,\n",
       " 176.692,\n",
       " 0.849498,\n",
       " 89.1978,\n",
       " 176.686,\n",
       " 0.470629,\n",
       " 78.8338,\n",
       " 368.575,\n",
       " 0.604896,\n",
       " 78.7628,\n",
       " 369.822,\n",
       " 0.732693,\n",
       " 89.2677,\n",
       " 369.798,\n",
       " 0.880098,\n",
       " 108.815,\n",
       " 360.767,\n",
       " 0.473532,\n",
       " 111.477,\n",
       " 360.792,\n",
       " 0.533813,\n",
       " 103.603,\n",
       " 360.751,\n",
       " 0.743098]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=data['people'][0]['pose_keypoints_2d']\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n"
     ]
    }
   ],
   "source": [
    "temp=np.array(temp)\n",
    "temp=temp.reshape(25,3)\n",
    "temp=temp.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.array(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peoples_short_000000000000_keypoints.json\n",
      "peoples_short_000000000001_keypoints.json\n",
      "peoples_short_000000000002_keypoints.json\n",
      "peoples_short_000000000003_keypoints.json\n",
      "peoples_short_000000000004_keypoints.json\n",
      "peoples_short_000000000005_keypoints.json\n",
      "peoples_short_000000000006_keypoints.json\n",
      "peoples_short_000000000007_keypoints.json\n",
      "peoples_short_000000000008_keypoints.json\n",
      "peoples_short_000000000009_keypoints.json\n",
      "peoples_short_000000000010_keypoints.json\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    num='000000000000'+str(i)\n",
    "    num=num[-12:]\n",
    "    temp_name='peoples_short_'+num+'_keypoints.json'\n",
    "    print(temp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
